import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({ temperature: 0.2, modelName: "gpt-4o" });

const prompt = ChatPromptTemplate.fromMessages([
    [
        "system",
        `
Báº¡n lÃ  trá»£ lÃ½ há»§y Ä‘Æ¡n hÃ ng.

ğŸ¯ Má»¥c tiÃªu: TÃ¬m vÃ  trÃ­ch xuáº¥t "email" vÃ  "mÃ£ Ä‘Æ¡n hÃ ng (orderId)" tá»« há»™i thoáº¡i cá»§a ngÆ°á»i dÃ¹ng.

ğŸ“¥ Äáº§u vÃ o: danh sÃ¡ch tin nháº¯n trÆ°á»›c Ä‘Ã³ (tin nháº¯n cá»§a user vÃ  bot).

ğŸ“¤ Äáº§u ra: chá»‰ tráº£ vá» chuá»—i JSON há»£p lá»‡ cÃ³ 2 field: "email" vÃ  "orderId". Náº¿u khÃ´ng cÃ³, tráº£ vá» null cho field Ä‘Ã³
ğŸš« KHÃ”NG tráº£ lá»i gÃ¬ thÃªm ngoÃ i JSON.
    `,
    ],
    new MessagesPlaceholder("messages"),
]);


const chain = prompt.pipe(model).pipe(async (output) => {
    try {
        console.log("Chain");
        console.log("Output:", output.content);
        const json = JSON.parse(output.content);
        console.log("JSON:", json);
        return {
            email: json.email,
            orderId: json.orderId,
            messages: output.response_messages,
        };
    } catch {
        return { messages: output.response_messages };
    }
});

export async function cancelLLM(state) {
    console.log("ğŸ“ Entered: cancelLLM");
    const result = await chain.invoke({ messages: state.messages });

    return {
        ...state,
        temp_email: result.email ?? state.temp_email,
        orderId: result.orderId ?? state.orderId,
        messages: [...state.messages, ...(result.messages || [])],
        current_step: "CheckInfo",
    };
}
